import tensorflow as tf
import matplotlib.pyplot as plt
from tensorflow.keras.models import Model
from tensorflow.keras.callbacks import ModelCheckpoint
from tensorflow.keras.applications.resnet50 import ResNet50

from google.colab import drive

from google.colab import drive
drive.mount('/content/drive')

# Preproccesing the dataset
train_ds, val_ds = tf.keras.utils.image_dataset_from_directory(
    "/content/drive/MyDrive/project sem 6 CVD detection/ecg_image_classification",
    labels="inferred",
    label_mode="int",
    class_names=["ECG Images of Myocardial Infarction Patients (240x12=2880)",
                 "ECG Images of Patient that have abnormal heartbeat (233x12=2796)",
                 "ECG Images of Patient that have History of MI (172x12=2064)",
                 "Normal Person ECG Images (284x12=3408)"],
    color_mode="rgb",
    batch_size=32,
    image_size=(182, 256),
    shuffle=True,
    seed=2,
    validation_split=0.1,
    subset="both",
    interpolation="area",
    crop_to_aspect_ratio=True,
 )

def normalize(image, labels):
  """Normalize training/test set"""
  return (tf.cast(image, tf.float32)/255.0, labels)
norm_train_ds = train_ds.map(normalize)
norm_val_ds = val_ds.map(normalize)

# Verify the normalization
i_2 = norm_train_ds.as_numpy_iterator()
image, label = i_2.next()
print(f"Image Shape: {image.shape}, Label Shape: {label.shape}")

# Show the image to see if the resize algorithem keep the crucial information or not
plt.imshow(i_2.next()[0][0], interpolation="nearest")

def vit_block(x):
  """ Define the transformer layer blocks for vit model"""
  prenorm = x
  x = tf.keras.layers.LayerNormalization()(x) # Add pre self attention layer norm
  x = tf.keras.layers.MultiHeadAttention(num_heads=32, key_dim=16, value_dim=16)(x, x) # Make a self attention layer(with 4 heads)
  x = prenorm + x # Residual(Use it so model predictes only the changes insead of the whole information)
  prenorm2 = x # Store the output before normalization
  x = tf.keras.layers.LayerNormalization()(x) # Add pre mlp layer norm
  x = mlp_block(x) + prenorm2 # Residual
  return x

def mlp_block(x):
  """define multi layer perceptron(dense*relu*dense)"""
  x = tf.keras.layers.Dense(units=2048, activation="relu")(x)
  x = tf.keras.layers.Dense(units=512, activation=None)(x)
  return x

# Define the model's artitecture
input = tf.keras.Input(shape=(182, 256, 3)) # Define the input neorons
x = tf.keras.layers.Conv2D(filters=512, kernel_size=(7, 8), strides=(7, 8))(input) # Patchify
x = tf.keras.layers.Reshape((26 * 32, 512))(x) # Flattening the inputs
# Add Positional information to the input because self attention is permutation invariant.
pos_emb = tf.keras.layers.Embedding(input_dim=26 * 32, output_dim=512)
pos_indices = tf.range(tf.shape(x)[1])
x = x + pos_emb(pos_indices)

for i in range(8): # Add vit blocks to the model
  x = vit_block(x)

x = tf.keras.layers.LayerNormalization()(x) # Final layer norm
# The magnitude of the residuals increase with the network depth, we need to do
# a finaal layer normalization to avoid really large activations here.
x = tf.keras.layers.Flatten()(x)  # Flatten the spatial dimensions
x = tf.keras.layers.Dense(units=4, activation=None)(x)  # Final classification layer

model = tf.keras.Model(inputs=input, outputs=x) # Make the model
model.summary()

# Compile the model
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4, beta_2=.99, weight_decay=1e0),
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=[tf.keras.metrics.SparseCategoricalAccuracy()]
              )

from tensorflow.keras.callbacks import ModelCheckpoint

checkpoint_callback = ModelCheckpoint(
    filepath='/content/drive/MyDrive/project sem 6 CVD detection/ecg_model_checkpoint',  # Specify the file to save the model
    save_best_only=False,            # Save only the best model
    monitor='val_loss',              # Monitor the validation loss
    mode='min',                      # Mode can be 'min' or 'max' depending on the monitored metric
    save_weights_only=False,         # Save the entire model, not just weights
    save_freq=5                      # Save every 5 epochs
)

history = model.fit(norm_train_ds, epochs=10, callbacks=[checkpoint_callback], validation_data=norm_val_ds) # Train the model



# Compile the model
model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])

# Generate random normal data for training and validation
train_data = np.random.normal(size=(1000, 26))  # Assuming 1000 samples with 26 features
train_labels = np.random.randint(2, size=(1000,))  # Binary labels for training data

validation_data = np.random.normal(size=(200, 26))  # Assuming 200 samples for validation
validation_labels = np.random.randint(2, size=(200,))  # Binary labels for validation data

# Now you can use your model with the fit method
history = model.fit(train_data, train_labels, epochs=50, validation_data=(validation_data, validation_labels))


# Evaluate the model on validation set
# Evaluate the model on validation set
validation_loss, validation_accuracy = model.evaluate(norm_val_ds, verbose=2)



# Evaluate the model on train set
model_loss, model_accuracy = model.evaluate(norm_train_ds, verbose= 2)

# Plot training history (loss and accuracy)
import matplotlib.pyplot as plt

plt.figure(figsize=(10, 4))

# Plot training & validation loss values
plt.subplot(121)
plt.plot(history.history['loss'], label='Train')
plt.plot(history.history['val_loss'], label='Validation')
plt.title('Model Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend()

# Plot training & validation accuracy values
plt.subplot(122)
plt.plot(history.history['sparse_categorical_accuracy'], label='Train')
plt.plot(history.history['val_sparse_categorical_accuracy'], label='Validation')
plt.title('Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend()

plt.tight_layout()
plt.show()

